/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loading pipeline components...:  29%|██▊       | 2/7 [00:03<00:09,  1.98s/it]Loading pipeline components...:  43%|████▎     | 3/7 [00:04<00:05,  1.26s/it]Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:01,  1.19it/s]Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.34it/s]
0 is assigned horse :: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
0 is doing horse :: 0
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:02<01:49,  2.23s/it]  4%|▍         | 2/50 [00:03<01:20,  1.68s/it]  6%|▌         | 3/50 [00:05<01:23,  1.77s/it]  8%|▊         | 4/50 [00:07<01:23,  1.82s/it] 10%|█         | 5/50 [00:09<01:22,  1.84s/it] 12%|█▏        | 6/50 [00:11<01:21,  1.86s/it] 14%|█▍        | 7/50 [00:12<01:20,  1.87s/it] 16%|█▌        | 8/50 [00:14<01:18,  1.88s/it] 18%|█▊        | 9/50 [00:16<01:17,  1.89s/it] 20%|██        | 10/50 [00:18<01:15,  1.89s/it] 22%|██▏       | 11/50 [00:20<01:14,  1.90s/it] 24%|██▍       | 12/50 [00:22<01:12,  1.90s/it] 26%|██▌       | 13/50 [00:24<01:10,  1.91s/it] 28%|██▊       | 14/50 [00:26<01:08,  1.91s/it] 30%|███       | 15/50 [00:28<01:06,  1.91s/it] 32%|███▏      | 16/50 [00:30<01:05,  1.91s/it] 34%|███▍      | 17/50 [00:32<01:03,  1.91s/it] 36%|███▌      | 18/50 [00:33<01:01,  1.92s/it] 38%|███▊      | 19/50 [00:35<00:59,  1.92s/it] 40%|████      | 20/50 [00:37<00:57,  1.92s/it] 42%|████▏     | 21/50 [00:39<00:55,  1.92s/it] 44%|████▍     | 22/50 [00:41<00:53,  1.92s/it] 46%|████▌     | 23/50 [00:43<00:51,  1.91s/it] 48%|████▊     | 24/50 [00:45<00:49,  1.91s/it] 50%|█████     | 25/50 [00:47<00:47,  1.91s/it] 52%|█████▏    | 26/50 [00:49<00:45,  1.91s/it] 54%|█████▍    | 27/50 [00:51<00:44,  1.91s/it] 56%|█████▌    | 28/50 [00:53<00:42,  1.92s/it] 58%|█████▊    | 29/50 [00:55<00:40,  1.92s/it] 60%|██████    | 30/50 [00:56<00:38,  1.92s/it] 62%|██████▏   | 31/50 [00:58<00:36,  1.92s/it] 64%|██████▍   | 32/50 [01:00<00:34,  1.92s/it] 66%|██████▌   | 33/50 [01:02<00:32,  1.93s/it]slurmstepd: error: *** JOB 702 ON cn3 CANCELLED AT 2024-09-04T03:27:10 ***
W0904 03:27:10.379000 139868734900032 torch/distributed/elastic/agent/server/api.py:688] Received 15 death signal, shutting down workers
W0904 03:27:10.381000 139868734900032 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 792659 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/rishubhp/miniconda3/envs/vaibhav/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1097, in launch_command
    multi_gpu_launcher(args)
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/accelerate/commands/launch.py", line 734, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/rishubhp/miniconda3/envs/vaibhav/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 792592 got signal: 15
