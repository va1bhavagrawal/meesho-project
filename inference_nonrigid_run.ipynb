{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1be97-2c13-4965-9ad1-945aa8ac73d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADOBE CONFIDENTIAL\n",
    "Copyright 2024 Adobe\n",
    "All Rights Reserved.\n",
    "NOTICE: All information contained herein is, and remains\n",
    "the property of Adobe and its suppliers, if any. The intellectual\n",
    "and technical concepts contained herein are proprietary to Adobe \n",
    "and its suppliers and are protected by all applicable intellectual \n",
    "property laws, including trade secret and copyright laws. \n",
    "Dissemination of this information or reproduction of this material is \n",
    "strictly forbidden unless prior written permission is obtained from Adobe.\n",
    "\"\"\"\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "from lora_diffusion import patch_pipe\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import os.path as osp \n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from training_scripts.continuous_word_mlp import continuous_word_mlp\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "# cur_model = \"nonrigid-run\" \n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\n",
    "    \"cuda\"\n",
    ")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "# patch_pipe(\n",
    "#     pipe,\n",
    "#     \"ckpts/\" + cur_model + \"_sd.safetensors\",\n",
    "#     patch_text=True,\n",
    "#     patch_ti=True,\n",
    "#     patch_unet=True,\n",
    "# )\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    osp.join(f\"ckpts/blue_truck/lora_weight_e299_s30000.safetensors\"), \n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "\n",
    "continuous_word_model = continuous_word_mlp(input_size = 2, output_size = 1024)\n",
    "continuous_word_model.load_state_dict(torch.load(\"ckpts/blue_truck/mlp299_s30000.pt\"))\n",
    "continuous_word_model.eval()\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976958e0-2b4f-48b0-9fb1-d1d4d0a10608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "cur_token = 'sks'\n",
    "\n",
    "img_list = []\n",
    "\n",
    "corresponding_emb = tokenizer(cur_token,\n",
    "        padding=\"do_not_pad\", \\\n",
    "         truncation=True, \\\n",
    "         max_length = tokenizer.model_max_length).input_ids[1]\n",
    "\n",
    "interpolation_gap = 5 \n",
    "for idx in range(interpolation_gap):\n",
    "\n",
    "    p = torch.Tensor([idx/(2 * math.pi)])\n",
    "    # 15 the pre-defined number to normalize the attributes between 0 to 0.5\n",
    "    x = torch.Tensor([torch.sin(2 * torch.pi * p), torch.cos(2 * torch.pi * p)]).cuda()\n",
    "    continuous_word_model = continuous_word_model.cuda()\n",
    "    mlp_emb = continuous_word_model(torch.unsqueeze(x, dim=0)).squeeze(0)\n",
    "    \n",
    "    \"\"\"Replacing the rare token embeddings with the outputs of the MLP\"\"\"  \n",
    "    with torch.no_grad():\n",
    "        pipe.text_encoder.get_input_embeddings().weight[corresponding_emb] = mlp_emb\n",
    "\n",
    "    torch.manual_seed(50)\n",
    "    prompt = 'a sks photo of a bnha pickup truck'  \n",
    "    # image = pipe(prompt, negative_prompt=\"bnha, worst quality\", num_inference_steps=50, guidance_scale=6).images[0]\n",
    "    image = pipe(prompt, negative_prompt=\"\", num_inference_steps=50, guidance_scale=6).images[0]\n",
    "    img_list.append(image)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2028c3-fbd9-49ba-82ec-41128424d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "# img_list = img_list[:-1]\n",
    "for i in range(len(img_list)):\n",
    "    fig.add_subplot(1, len(img_list), i + 1)\n",
    "    plt.imshow(np.array(img_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5c8c8-7d0d-43a9-95d4-e2261df113a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
